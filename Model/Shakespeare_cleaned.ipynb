{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import os\n",
    "import sys\n",
    "import nltk\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel('/Users/anamolpundle/Documents/Insight/NOS_database_table_of_contents.xls', sheetname='phase1_table_of_contents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" List of contested authors\"\"\"\n",
    "\n",
    "author_list = ['Greene, Robert', 'Marlowe, Christopher', 'Shakespeare', \n",
    "               'Peele, George', 'Kyd, Thomas', \n",
    "               'Nash, Thomas', 'Watson, Thomas', 'Munday', 'Lodge', \n",
    "               'Lyly', 'Drayton', 'Achelley', 'Chettle'] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Replace Shakespeare by 'good' plays only \"\"\"\n",
    "\n",
    "all_texts = []\n",
    "for ctr, author in enumerate(author_list):\n",
    "    if (author == 'Shakespeare'):\n",
    "        author = 'Shakesgood'\n",
    "    author = author.replace(\" \", \"\")\n",
    "    path = '/Users/anamolpundle/Documents/Insight/MLDatatxt/' + author + '/'\n",
    "    all_author = ''\n",
    "    for file in os.listdir(path):\n",
    "        text = open(path + file, 'r').read()\n",
    "        all_author = all_author + text\n",
    "    all_texts.append(all_author)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Split text into documents of 3000 words each \"\"\"\n",
    "\n",
    "\n",
    "words_per_doc = 3000\n",
    "all_docs = []\n",
    "\n",
    "for ctr, text in enumerate(all_texts):\n",
    "    text_words = nltk.tokenize.word_tokenize(text)\n",
    "    docs = []\n",
    "    \n",
    "    for i in range(0,len(text_words),words_per_doc):\n",
    "        if (i + words_per_doc >= len(text_words)):\n",
    "            docs.append(\" \".join(text_words[i:]))\n",
    "        else:\n",
    "            docs.append(\" \".join(text_words[i:i+words_per_doc]))\n",
    "    docs[-2] = docs[-1] + docs[-2]\n",
    "    del docs[-1]\n",
    "    all_docs.append(docs)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Save authors into author_array and texts into text_array\"\"\"\n",
    "\"\"\" Remove Achelly, Kyd and Chettle\"\"\"\n",
    "\n",
    "author_array = []\n",
    "text_array = []\n",
    "all_text = \"\"\n",
    "for ctr, author in enumerate(all_docs):\n",
    "    if ((author_list[ctr] != 'Achelley') and (author_list[ctr] !='Chettle') and (author_list[ctr] !='Kyd, Thomas')):\n",
    "        for text in author:\n",
    "            author_array.append(author_list[ctr])\n",
    "            text_array.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Split into train and holdout data sets. Imbalance not corrected \"\"\"\n",
    "\n",
    "X_train, X_holdout, y_train, y_holdout = train_test_split(text_array, author_array, test_size = 0.15, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Split train into train and validation set. Imbalance not corrected\"\"\"\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_train_text_data = \"\"\n",
    "for doc in X_train:\n",
    "    all_train_text_data = all_train_text_data + ' ' + doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" TFIDF instead of bag of words. Takes a while (~10 mins) \"\"\"\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "def tokenize(text):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(text.lower())\n",
    "    stems = []\n",
    "    for item in tokens:\n",
    "        stems.append(PorterStemmer().stem(item))\n",
    "    return stems\n",
    "\n",
    "X_train_no_digits = ''.join([i for i in all_train_text_data if not i.isdigit()])\n",
    "all_words = tokenize(X_train_no_digits)\n",
    "\n",
    "tfidf = TfidfVectorizer(tokenizer=tokenize, min_df=3 , max_df = 0.5, ngram_range=(1,4), max_features = 400000)\n",
    "\n",
    "train_tfs = tfidf.fit_transform(X_train)\n",
    "test_tfs = tfidf.transform(X_test)\n",
    "holdout_tfs = tfidf.transform(X_holdout)\n",
    "\n",
    "X_train_df = pd.DataFrame(train_tfs.toarray())\n",
    "X_test_df = pd.DataFrame(test_tfs.toarray())\n",
    "X_holdout_df = pd.DataFrame(holdout_tfs.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
